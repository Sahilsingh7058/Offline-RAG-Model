# ğŸ§â€â™‚ï¸ OfflineGenie  
### Your 100% Private, Offline-First RAG (Retrieval-Augmented Generation) Application  

![Static Badge](https://img.shields.io/badge/Built%20With-FastAPI-blue?style=flat-square)
![Static Badge](https://img.shields.io/badge/Frontend-React%20%2B%20Tailwind-green?style=flat-square)
![Static Badge](https://img.shields.io/badge/LLM-Llama%203-orange?style=flat-square)
![Static Badge](https://img.shields.io/badge/License-MIT-lightgrey?style=flat-square)

---

## ğŸ§© Overview  

**OfflineGenie** is a full-stack, **local-first RAG (Retrieval-Augmented Generation)** app.  
It allows you to upload your personal documents (PDFs, CSVs, etc.) and chat with them using a **powerful large language model running entirely on your machine**.  

ğŸ›¡ï¸ **Your files and conversations never leave your computer**, ensuring 100% privacy.

---

## âœ¨ Core Features  

- âš¡ **100% Offline RAG** â€” Entire chat and indexing pipeline (file parsing, embedding, and generation) runs locally.  
- ğŸ§  **Local LLM Integration** â€” Powered by **Llama 3** (or any compatible model) via **Ollama**.  
- ğŸ”’ **Private & Secure** â€” No data ever leaves your system.  
- ğŸ“š **Multimodel File Support** â€” Processes **PDFs** and **CSVs** effortlessly.  
- ğŸ–¥ï¸ **Modern UI** â€” Built with **React + Tailwind CSS** with light/dark modes.  
- ğŸŒ *(Optional)* **Online Enhancements (via Gemini API):**  
  - âœ¨ Document Summarization  
  - âœ¨ Query Augmentation  
  - âœ¨ Insight Extraction  

---

## ğŸ› ï¸ Tech Stack  

| Component | Technology |
|------------|-------------|
| **Frontend** | React, Tailwind CSS, Lucide-React |
| **Backend** | Python 3, FastAPI |
| **AI Orchestration** | LangChain |
| **Local LLM** | Ollama (serving Llama 3) |
| **Embedding Model** | all-MiniLM-L6-v2 (Sentence Transformers) |
| **Vector Database** | FAISS (Facebook AI Similarity Search) |

---

## ğŸš€ Getting Started  

### ğŸ§° Prerequisites  

You must have the following installed:
- [Node.js](https://nodejs.org/) (LTS version recommended)  
- [Python 3.10+](https://www.python.org/downloads/)  
- [Ollama](https://ollama.com)  

Pull the **Llama 3** model:  
```bash
ollama pull llama3

âš™ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository

git clone https://github.com/YourUsername/OfflineGenie.git
cd OfflineGenie


2ï¸âƒ£ Start the Backend Server (Terminal 1)
cd backend
python -m venv venv

Install dependencies:
pip install -r requirements.txt

Run the FastAPI server:
python -m uvicorn main:app --reload

Backend available at â†’ http://127.0.0.1:8000

3ï¸âƒ£ Start the Frontend App (Terminal 2)
cd frontend
npm install
npm start

Frontend available at â†’ http://localhost:3000

ğŸ“ Folder Structure
OfflineGenie/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ routes/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â””â”€â”€ README.md

ğŸ’¡ How It Works

Upload Document â€” PDFs/CSVs are parsed locally.

Generate Embeddings â€” Each text chunk is converted into vectors via Sentence Transformers.

Store in FAISS â€” Embeddings stored locally for fast retrieval.

Ask a Question â€” User query converted into vector and matched.

RAG Pipeline â€” Retrieved chunks + query â†’ sent to Llama 3.

AI Response â€” Generated locally and displayed in chat UI.

